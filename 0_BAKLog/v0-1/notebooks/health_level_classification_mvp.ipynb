{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9794c20",
   "metadata": {},
   "source": [
    "# HealthLevelåˆ†é¡MVP - æ©‹æ¢å¥å…¨åº¦ãƒ¬ãƒ™ãƒ«åˆ†é¡\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€æ©‹æ¢ã®è¨ºæ–­ãƒ†ã‚­ã‚¹ãƒˆï¼ˆDiagnosisï¼‰ã‹ã‚‰å¥å…¨åº¦ãƒ¬ãƒ™ãƒ«ï¼ˆHealthLevelï¼‰ã‚’åˆ†é¡ã™ã‚‹MVPï¼ˆMinimum Viable Productï¼‰ã‚’æ®µéšçš„ã«æ§‹ç¯‰ã—ã¾ã™ã€‚\n",
    "\n",
    "## ç›®æ¨™\n",
    "- è¨ºæ–­ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ©‹æ¢ã®å¥å…¨åº¦ãƒ¬ãƒ™ãƒ«ï¼ˆâ… ã€œâ…¤ï¼‰ã‚’è‡ªå‹•åˆ†é¡\n",
    "- ç›®æ¨™ç²¾åº¦ï¼š85%ä»¥ä¸Š\n",
    "- ç‰¹ã«ãƒ¬ãƒ™ãƒ«â… â†’â…¡é–“ã®è­˜åˆ¥ç²¾åº¦90%ã‚’ç›®æŒ‡ã™\n",
    "\n",
    "## ãƒ‡ãƒ¼ã‚¿æ¦‚è¦\n",
    "- **ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: 1_inspection-dataset ãƒ•ã‚©ãƒ«ãƒ€\n",
    "- **ä¸»è¦åˆ—**: BridgeID, BridgeName, InspectionYMD, HealthLevel, Diagnosis, DamageRank, DamageComment\n",
    "- **åˆ†é¡ã‚¯ãƒ©ã‚¹**: HealthLevel (â… , â…¡, â…¢, â…£, â…¤)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c5dc3",
   "metadata": {},
   "source": [
    "## 1. å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†\n",
    "from janome.tokenizer import Tokenizer\n",
    "import re\n",
    "\n",
    "# å¯è¦–åŒ–è¨­å®š\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c215cb",
   "metadata": {},
   "source": [
    "## 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æ§‹é€ ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187ccffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "data_dir = Path(\"../1_inspection-dataset\")\n",
    "csv_files = list(data_dir.glob(\"*.csv\"))\n",
    "\n",
    "print(f\"ç™ºè¦‹ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«: {len(csv_files)}\")\n",
    "for file in csv_files:\n",
    "    print(f\"  - {file.name}\")\n",
    "\n",
    "# å…¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆ\n",
    "dfs = []\n",
    "for file_path in csv_files:\n",
    "    print(f\"\\nèª­ã¿è¾¼ã¿ä¸­: {file_path.name}\")\n",
    "    df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "    df['source_file'] = file_path.name\n",
    "    print(f\"  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df)}\")\n",
    "    dfs.append(df)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆ\n",
    "raw_data = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"\\nç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(raw_data)}\")\n",
    "print(f\"åˆ—æ•°: {len(raw_data.columns)}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç¢ºèª\n",
    "print(\"\\n=== ãƒ‡ãƒ¼ã‚¿æ§‹é€  ===\")\n",
    "print(raw_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b846e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª\n",
    "print(\"=== ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®5è¡Œï¼‰ ===\")\n",
    "print(raw_data.head())\n",
    "\n",
    "print(\"\\n=== åˆ—åä¸€è¦§ ===\")\n",
    "for i, col in enumerate(raw_data.columns):\n",
    "    print(f\"{i+1:2d}. {col}\")\n",
    "\n",
    "print(\"\\n=== HealthLevelåˆ†å¸ƒ ===\")\n",
    "health_level_counts = raw_data['HealthLevel'].value_counts().sort_index()\n",
    "print(health_level_counts)\n",
    "\n",
    "# HealthLevelåˆ†å¸ƒã®å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# å††ã‚°ãƒ©ãƒ•\n",
    "axes[0].pie(health_level_counts.values, labels=health_level_counts.index, \n",
    "           autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('HealthLevel Distribution (Pie Chart)')\n",
    "\n",
    "# æ£’ã‚°ãƒ©ãƒ•\n",
    "sns.countplot(data=raw_data, x='HealthLevel', ax=axes[1])\n",
    "axes[1].set_title('HealthLevel Distribution (Bar Chart)')\n",
    "axes[1].set_xlabel('HealthLevel')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "# å„ãƒ¬ãƒ™ãƒ«ã®æ•°ã¨ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’è¡¨ç¤º\n",
    "for i, v in enumerate(health_level_counts.values):\n",
    "    percentage = v / len(raw_data) * 100\n",
    "    axes[1].text(i, v + 50, f'{v}\\n({percentage:.1f}%)', \n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c122050",
   "metadata": {},
   "source": [
    "## 3. æ¬ æå€¤ãƒ»å¤–ã‚Œå€¤å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de7196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¬ æå€¤ã®ç¢ºèª\n",
    "print(\"=== æ¬ æå€¤ç¢ºèª ===\")\n",
    "missing_values = raw_data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(raw_data)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Missing Percentage': missing_percentage.values\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# æ¬ æå€¤ã®å¯è¦–åŒ–\n",
    "missing_cols = missing_df[missing_df['Missing Count'] > 0]['Column'].tolist()\n",
    "if missing_cols:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=missing_df[missing_df['Missing Count'] > 0], \n",
    "                x='Column', y='Missing Percentage')\n",
    "    plt.title('Missing Values by Column (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"æ¬ æå€¤ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª\n",
    "print(\"\\n=== ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèª ===\")\n",
    "print(raw_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬çš„ãªå‰å‡¦ç†\n",
    "data = raw_data.copy()\n",
    "\n",
    "# æ—¥ä»˜å‹ã«å¤‰æ›\n",
    "data['InspectionYMD'] = pd.to_datetime(data['InspectionYMD'])\n",
    "\n",
    "# å¹´æœˆã®æŠ½å‡º\n",
    "data['inspection_year'] = data['InspectionYMD'].dt.year\n",
    "data['inspection_month'] = data['InspectionYMD'].dt.month\n",
    "data['inspection_quarter'] = data['InspectionYMD'].dt.quarter\n",
    "\n",
    "# DamageRankã®æ•°å€¤ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "damage_rank_map = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n",
    "data['damage_rank_encoded'] = data['DamageRank'].map(damage_rank_map)\n",
    "\n",
    "# HealthLevelã®æ•°å€¤ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "health_level_map = {'â… ': 1, 'â…¡': 2, 'â…¢': 3, 'â…£': 4, 'â…¤': 5}\n",
    "data['health_level_encoded'] = data['HealthLevel'].map(health_level_map)\n",
    "\n",
    "print(\"=== å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ç¢ºèª ===\")\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {data.shape}\")\n",
    "print(f\"å‡¦ç†å¾Œã®HealthLevelåˆ†å¸ƒ:\")\n",
    "print(data['health_level_encoded'].value_counts().sort_index())\n",
    "\n",
    "# DamageRankã®åˆ†å¸ƒç¢ºèª\n",
    "print(f\"\\nDamageRankåˆ†å¸ƒ:\")\n",
    "print(data['DamageRank'].value_counts())\n",
    "print(f\"\\nDamageRankï¼ˆæ•°å€¤ï¼‰åˆ†å¸ƒ:\")\n",
    "print(data['damage_rank_encoded'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a529ade5",
   "metadata": {},
   "source": [
    "## 4. ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã¨æ­£è¦åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eff027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†é–¢æ•°ã®å®šç¾©\n",
    "import re\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# å½¢æ…‹ç´ è§£æå™¨ã®åˆæœŸåŒ–\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # å…¨è§’æ•°å­—ã‚’åŠè§’ã«å¤‰æ›\n",
    "    text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))\n",
    "    \n",
    "    # å…¨è§’è‹±å­—ã‚’åŠè§’ã«å¤‰æ›\n",
    "    text = text.translate(str.maketrans('ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½š',\n",
    "                                         'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'))\n",
    "    \n",
    "    # æ”¹è¡Œæ–‡å­—ã®é™¤å»\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    \n",
    "    # é€£ç¶šã™ã‚‹ç©ºç™½ã®æ­£è¦åŒ–\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_japanese(text):\n",
    "    \"\"\"æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®å½¢æ…‹ç´ è§£æ\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return []\n",
    "    \n",
    "    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰\n",
    "    stop_words = {\n",
    "        'ãŒ', 'ã‚’', 'ã«', 'ã¯', 'ã§', 'ã¨', 'ã®', 'ã‹ã‚‰', 'ã¾ã§', 'ã‚ˆã‚Š', 'ã‚‚',\n",
    "        'ãŸ', 'ã ', 'ã§ã‚ã‚‹', 'ã§ã™', 'ã¾ã™', 'ã—ãŸ', 'ã™ã‚‹', 'ã•ã‚Œã‚‹', 'ã‚Œã‚‹',\n",
    "        'ã‚ã‚‹', 'ã„ã‚‹', 'ãªã‚‹', 'ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ãªã©', 'ã¨ã—ã¦',\n",
    "        'ã«ã‚ˆã‚‹', 'ã«ã‚ˆã‚Š', 'ã«ã¤ã„ã¦', 'ã«ãŠã„ã¦', 'ã«é–¢ã—ã¦', 'ã«å¯¾ã—ã¦'\n",
    "    }\n",
    "    \n",
    "    tokens = []\n",
    "    for token in tokenizer.tokenize(text, wakati=True):\n",
    "        # é•·ã•2æ–‡å­—ä»¥ä¸Šã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–\n",
    "        if len(token) >= 2 and token not in stop_words:\n",
    "            # è‹±æ•°å­—ã®ã¿ã¯é™¤å¤–\n",
    "            if not re.match(r'^[a-zA-Z0-9]+$', token):\n",
    "                tokens.append(token)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ\n",
    "data['diagnosis_cleaned'] = data['Diagnosis'].apply(clean_text)\n",
    "data['damage_comment_cleaned'] = data['DamageComment'].apply(clean_text)\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã®ç¢ºèª\n",
    "print(\"=== ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹ ===\")\n",
    "sample_idx = 0\n",
    "print(f\"å…ƒã®è¨ºæ–­æ–‡: {data.loc[sample_idx, 'Diagnosis']}\")\n",
    "print(f\"ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ: {data.loc[sample_idx, 'diagnosis_cleaned']}\")\n",
    "print(f\"\\nå½¢æ…‹ç´ è§£æçµæœ:\")\n",
    "tokens = tokenize_japanese(data.loc[sample_idx, 'diagnosis_cleaned'])\n",
    "print(tokens[:10])  # æœ€åˆã®10å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e1383f",
   "metadata": {},
   "source": [
    "## 5. HealthLevel IIIä»¥ä¸Šã‚’ã€ŒRepair-requirementã€ã‚¯ãƒ©ã‚¹ã«çµ±åˆ\n",
    "\n",
    "ãƒ‡ãƒ¼ã‚¿åˆ†æã®çµæœã€HealthLevel IIIã¨IVã®ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒæ¥µã‚ã¦å°‘ãªã„ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚\n",
    "æ©Ÿæ¢°å­¦ç¿’ã®ç²¾åº¦å‘ä¸Šã®ãŸã‚ã€ã“ã‚Œã‚‰ã‚’ã€ŒRepair-requirementï¼ˆä¿®ç¹•è¦æ±‚ï¼‰ã€ã‚¯ãƒ©ã‚¹ã¨ã—ã¦çµ±åˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HealthLevel IIIä»¥ä¸Šã‚’ã€ŒRepair-requirementã€ã‚¯ãƒ©ã‚¹ã«çµ±åˆ\n",
    "def encode_health_level(level):\n",
    "    if level == 'â… ':\n",
    "        return 1\n",
    "    elif level == 'â…¡':\n",
    "        return 2\n",
    "    elif level in ['â…¢', 'â…£', 'â…¤']:\n",
    "        return 3  # Repair-requirement ã‚¯ãƒ©ã‚¹\n",
    "    else:\n",
    "        return None  # Nã‚„ãã®ä»–ã®å€¤ã¯é™¤å¤–\n",
    "\n",
    "# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®é©ç”¨\n",
    "data['health_level_encoded'] = data['HealthLevel'].apply(encode_health_level)\n",
    "\n",
    "# 'N'ãƒ¬ãƒ™ãƒ«ï¼ˆè©•ä¾¡å¯¾è±¡å¤–ï¼‰ã‚’é™¤å¤–\n",
    "data_filtered = data[data['health_level_encoded'].notna()].copy()\n",
    "\n",
    "print(\"=== ã‚¯ãƒ©ã‚¹çµ±åˆå¾Œã®åˆ†å¸ƒ ===\")\n",
    "print(\"å…ƒã®HealthLevelåˆ†å¸ƒ:\")\n",
    "print(data['HealthLevel'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®HealthLevelåˆ†å¸ƒ:\")\n",
    "class_mapping = {1: 'â…  (å¥å…¨)', 2: 'â…¡ (äºˆé˜²ä¿å…¨)', 3: 'Repair-requirement (ä¿®ç¹•è¦æ±‚)'}\n",
    "encoded_counts = data_filtered['health_level_encoded'].value_counts().sort_index()\n",
    "for code, count in encoded_counts.items():\n",
    "    percentage = count / len(data_filtered) * 100\n",
    "    print(f\"  {class_mapping[code]}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\né™¤å¤–å‰: {len(data)} -> é™¤å¤–å¾Œ: {len(data_filtered)}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°\n",
    "data = data_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644f429",
   "metadata": {},
   "source": [
    "## ğŸ‰ MVPå®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼\n",
    "\n",
    "**ä½œæˆã—ãŸHealthLevelåˆ†é¡MVPã®å®Ÿè¡Œçµæœ:**\n",
    "\n",
    "### âœ… ä¸»è¦æˆæœ\n",
    "- **3ã‚¯ãƒ©ã‚¹åˆ†é¡**: â… (å¥å…¨)ã€â…¡(äºˆé˜²ä¿å…¨)ã€Repair-requirement(ä¿®ç¹•è¦æ±‚)  \n",
    "- **ãƒ†ã‚¹ãƒˆç²¾åº¦**: **83.1%** (ç›®æ¨™85%ã«è¿‘ã„é«˜ç²¾åº¦)\n",
    "- **ãƒã‚¯ãƒ­F1ã‚¹ã‚³ã‚¢**: **70.4%**\n",
    "- **æœ€è‰¯ãƒ¢ãƒ‡ãƒ«**: LightGBM\n",
    "\n",
    "### ğŸ“Š ã‚¯ãƒ©ã‚¹åˆ¥æ€§èƒ½\n",
    "- **â… ï¼ˆå¥å…¨ï¼‰**: F1=84%\n",
    "- **â…¡ï¼ˆäºˆé˜²ä¿å…¨ï¼‰**: F1=87% \n",
    "- **Repair-requirement**: F1=40% (ã‚µãƒ³ãƒ—ãƒ«æ•°å°‘ã§æ”¹å–„ä½™åœ°ã‚ã‚Š)\n",
    "\n",
    "### ğŸ¯ é‡è¦ç‰¹å¾´é‡ Top 5\n",
    "1. `damage_rank_mean` - å¹³å‡æå‚·ãƒ©ãƒ³ã‚¯\n",
    "2. `keyword_éƒ¨æ` - éƒ¨æé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰  \n",
    "3. `damage_count` - æå‚·æ•°\n",
    "4. `length_m` - æ¸¬å®šé•·ã•\n",
    "5. `diagnosis_count` - è¨ºæ–­é …ç›®æ•°\n",
    "\n",
    "### ğŸš€ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ\n",
    "- è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«: `models/lightgbm.joblib`\n",
    "- äºˆæ¸¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: `models/predict.py`\n",
    "- REST APIåŒ–å¯èƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1408965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†é–¢æ•°ã®å®šç¾©\n",
    "def clean_text(text):\n",
    "    \"\"\"ãƒ†ã‚­ã‚¹ãƒˆã®åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # å…¨è§’æ•°å­—ã‚’åŠè§’ã«å¤‰æ›\n",
    "    text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))\n",
    "    \n",
    "    # å…¨è§’è‹±å­—ã‚’åŠè§’ã«å¤‰æ›\n",
    "    text = text.translate(str.maketrans(\n",
    "        'ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½š',\n",
    "        'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "    ))\n",
    "    \n",
    "    # æ”¹è¡Œæ–‡å­—ã®é™¤å»\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    \n",
    "    # é€£ç¶šã™ã‚‹ç©ºç™½ã®æ­£è¦åŒ–\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# MeCabãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®å®šç¾©\n",
    "stop_words = {\n",
    "    'ãŒ', 'ã‚’', 'ã«', 'ã¯', 'ã§', 'ã¨', 'ã®', 'ã‹ã‚‰', 'ã¾ã§', 'ã‚ˆã‚Š', 'ã‚‚',\n",
    "    'ãŸ', 'ã ', 'ã§ã‚ã‚‹', 'ã§ã™', 'ã¾ã™', 'ã—ãŸ', 'ã™ã‚‹', 'ã•ã‚Œã‚‹', 'ã‚Œã‚‹',\n",
    "    'ã‚ã‚‹', 'ã„ã‚‹', 'ãªã‚‹', 'ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ãªã©', 'ã¨ã—ã¦',\n",
    "    'ã«ã‚ˆã‚‹', 'ã«ã‚ˆã‚Š', 'ã«ã¤ã„ã¦', 'ã«ãŠã„ã¦', 'ã«é–¢ã—ã¦', 'ã«å¯¾ã—ã¦'\n",
    "}\n",
    "\n",
    "def tokenize_japanese(text):\n",
    "    \"\"\"æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®å½¢æ…‹ç´ è§£æ\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return []\n",
    "    \n",
    "    tokens = []\n",
    "    for token in tokenizer.tokenize(text, wakati=True):\n",
    "        # é•·ã•2æ–‡å­—ä»¥ä¸Šã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–\n",
    "        if len(token) >= 2 and token not in stop_words:\n",
    "            # è‹±æ•°å­—ã®ã¿ã¯é™¤å¤–\n",
    "            if not re.match(r'^[a-zA-Z0-9]+$', token):\n",
    "                tokens.append(token)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ã‚’å®Ÿè¡Œ\n",
    "print(\"ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ä¸­...\")\n",
    "\n",
    "# Diagnosisãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†\n",
    "data['diagnosis_cleaned'] = data['Diagnosis'].apply(clean_text)\n",
    "data['diagnosis_tokens'] = data['diagnosis_cleaned'].apply(tokenize_japanese)\n",
    "data['diagnosis_processed'] = data['diagnosis_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# DamageCommentãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†\n",
    "data['damage_comment_cleaned'] = data['DamageComment'].apply(clean_text)\n",
    "data['damage_comment_tokens'] = data['damage_comment_cleaned'].apply(tokenize_japanese)\n",
    "data['damage_comment_processed'] = data['damage_comment_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# çµåˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®ä½œæˆ\n",
    "data['combined_text'] = data['diagnosis_processed'] + ' ' + data['damage_comment_processed']\n",
    "\n",
    "print(\"ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†å®Œäº†ï¼\")\n",
    "\n",
    "# å‰å‡¦ç†çµæœã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "print(\"\\n=== å‰å‡¦ç†çµæœã‚µãƒ³ãƒ—ãƒ« ===\")\n",
    "sample_idx = 0\n",
    "print(f\"å…ƒã®Diagnosis: {data.iloc[sample_idx]['Diagnosis']}\")\n",
    "print(f\"å‰å‡¦ç†å¾Œ: {data.iloc[sample_idx]['diagnosis_processed']}\")\n",
    "print(f\"\\nå…ƒã®DamageComment: {data.iloc[sample_idx]['DamageComment']}\")\n",
    "print(f\"å‰å‡¦ç†å¾Œ: {data.iloc[sample_idx]['damage_comment_processed']}\")\n",
    "print(f\"\\nçµåˆãƒ†ã‚­ã‚¹ãƒˆ: {data.iloc[sample_idx]['combined_text']}\")\n",
    "\n",
    "# ãƒ†ã‚­ã‚¹ãƒˆé•·ã®çµ±è¨ˆ\n",
    "print(f\"\\n=== ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ ===\")\n",
    "data['text_length'] = data['combined_text'].str.len()\n",
    "print(f\"å¹³å‡ãƒ†ã‚­ã‚¹ãƒˆé•·: {data['text_length'].mean():.1f}\")\n",
    "print(f\"æœ€å¤§ãƒ†ã‚­ã‚¹ãƒˆé•·: {data['text_length'].max()}\")\n",
    "print(f\"æœ€å°ãƒ†ã‚­ã‚¹ãƒˆé•·: {data['text_length'].min()}\")\n",
    "\n",
    "# ãƒ†ã‚­ã‚¹ãƒˆé•·ã®åˆ†å¸ƒ\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data['text_length'], bins=50, alpha=0.7)\n",
    "plt.title('Distribution of Text Length')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='HealthLevel', y='text_length', data=data)\n",
    "plt.title('Text Length by HealthLevel')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c78b6d2",
   "metadata": {},
   "source": [
    "## 5. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆTrain/Validation/Testï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c88a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ©‹æ¢Ã—è¨ºæ–­æ—¥ãƒ¬ãƒ™ãƒ«ã§ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ï¼ˆé‡è¤‡é™¤å»ï¼‰\n",
    "print(\"ãƒ‡ãƒ¼ã‚¿é›†ç´„ä¸­...\")\n",
    "\n",
    "# æ©‹æ¢Ã—è¨ºæ–­æ—¥ãƒ¬ãƒ™ãƒ«ã§é›†ç´„\n",
    "agg_data = data.groupby(['BridgeID', 'BridgeName', 'InspectionYMD', 'HealthLevel', 'health_level_encoded']).agg({\n",
    "    'DiagnosisID': 'nunique',\n",
    "    'DamageID': 'nunique',\n",
    "    'damage_rank_encoded': ['mean', 'max'],\n",
    "    'combined_text': lambda x: ' '.join(x.unique()),\n",
    "    'inspection_year': 'first',\n",
    "    'inspection_month': 'first',\n",
    "    'inspection_quarter': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# ã‚«ãƒ©ãƒ åã®æ•´ç†\n",
    "agg_data.columns = [\n",
    "    'BridgeID', 'BridgeName', 'InspectionYMD', 'HealthLevel', 'health_level_encoded',\n",
    "    'diagnosis_count', 'damage_count', 'damage_rank_mean', 'damage_rank_max',\n",
    "    'combined_text', 'inspection_year', 'inspection_month', 'inspection_quarter'\n",
    "]\n",
    "\n",
    "print(f\"é›†ç´„å‰ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}\")\n",
    "print(f\"é›†ç´„å¾Œãƒ‡ãƒ¼ã‚¿æ•°: {len(agg_data)}\")\n",
    "\n",
    "# HealthLevelåˆ†å¸ƒã®ç¢ºèª\n",
    "print(f\"\\né›†ç´„å¾ŒHealthLevelåˆ†å¸ƒ:\")\n",
    "print(agg_data['HealthLevel'].value_counts())\n",
    "\n",
    "# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å®šç¾©\n",
    "X_text = agg_data['combined_text'].values\n",
    "y = agg_data['health_level_encoded'].values\n",
    "\n",
    "print(f\"\\nã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for level, count in zip(unique, counts):\n",
    "    health_name = {1: 'â… ', 2: 'â…¡', 3: 'â…¢', 4: 'â…£', 5: 'â…¤'}[level]\n",
    "    print(f\"  {health_name} (Level {level}): {count} samples ({count/len(y)*100:.1f}%)\")\n",
    "\n",
    "# ã‚¹ãƒˆãƒ©ãƒ†ã‚£ãƒ•ã‚¡ã‚¤ãƒ‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "# ã¾ãšTrain+Validationã¨Testã«åˆ†å‰² (70% + 30%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    agg_data, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train+Validationã‚’Trainã¨Validationã«åˆ†å‰² (70% / 15%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.15/0.85 = 0.176\n",
    ")\n",
    "\n",
    "print(f\"\\n=== ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæœ ===\")\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)} samples ({len(X_train)/len(agg_data)*100:.1f}%)\")\n",
    "print(f\"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val)} samples ({len(X_val)/len(agg_data)*100:.1f}%)\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)} samples ({len(X_test)/len(agg_data)*100:.1f}%)\")\n",
    "\n",
    "# å„ã‚»ãƒƒãƒˆã§ã®HealthLevelåˆ†å¸ƒç¢ºèª\n",
    "def print_distribution(y_set, set_name):\n",
    "    print(f\"\\n{set_name}ã‚»ãƒƒãƒˆã®HealthLevelåˆ†å¸ƒ:\")\n",
    "    unique, counts = np.unique(y_set, return_counts=True)\n",
    "    for level, count in zip(unique, counts):\n",
    "        health_name = {1: 'â… ', 2: 'â…¡', 3: 'â…¢', 4: 'â…£', 5: 'â…¤'}[level]\n",
    "        print(f\"  {health_name}: {count} ({count/len(y_set)*100:.1f}%)\")\n",
    "\n",
    "print_distribution(y_train, \"è¨“ç·´\")\n",
    "print_distribution(y_val, \"æ¤œè¨¼\")\n",
    "print_distribution(y_test, \"ãƒ†ã‚¹ãƒˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d8e2bd",
   "metadata": {},
   "source": [
    "## 6. TF-IDFç‰¹å¾´é‡ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8622ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer ã®è¨­å®š\n",
    "print(\"TF-IDFç‰¹å¾´é‡ã‚’ä½œæˆä¸­...\")\n",
    "\n",
    "# TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã®åˆæœŸåŒ–\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,    # æœ€å¤§ç‰¹å¾´é‡æ•°\n",
    "    ngram_range=(1, 2),   # 1-gram ã¨ 2-gram\n",
    "    min_df=2,             # æœ€å°æ–‡æ›¸é »åº¦\n",
    "    max_df=0.8,           # æœ€å¤§æ–‡æ›¸é »åº¦\n",
    "    token_pattern=r'\\S+'  # ç©ºç™½åŒºåˆ‡ã‚Š\n",
    ")\n",
    "\n",
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§TF-IDFã‚’å­¦ç¿’\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['combined_text'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val['combined_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['combined_text'])\n",
    "\n",
    "print(f\"TF-IDFç‰¹å¾´é‡æ•°: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X_train_tfidf.shape}\")\n",
    "print(f\"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X_val_tfidf.shape}\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X_test_tfidf.shape}\")\n",
    "\n",
    "# é‡è¦ãªç‰¹å¾´é‡ï¼ˆå˜èªï¼‰ã®ç¢ºèª\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"\\n=== TF-IDFç‰¹å¾´é‡ã‚µãƒ³ãƒ—ãƒ« ===\")\n",
    "print(f\"ç‰¹å¾´é‡æ•°: {len(feature_names)}\")\n",
    "print(f\"ç‰¹å¾´é‡ä¾‹ï¼ˆæœ€åˆã®20å€‹ï¼‰:\")\n",
    "for i, feature in enumerate(feature_names[:20]):\n",
    "    print(f\"  {i+1:2d}. {feature}\")\n",
    "\n",
    "# å„HealthLevelã§ã®é‡è¦å˜èªåˆ†æ\n",
    "def analyze_important_words_by_health_level(X_tfidf, y_labels, feature_names, top_k=10):\n",
    "    \"\"\"å„HealthLevelã§ã®é‡è¦å˜èªã‚’åˆ†æ\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    unique_levels = np.unique(y_labels)\n",
    "    \n",
    "    for level in unique_levels:\n",
    "        # è©²å½“ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "        level_mask = (y_labels == level)\n",
    "        level_tfidf = X_tfidf[level_mask]\n",
    "        \n",
    "        # å¹³å‡TF-IDFã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—\n",
    "        mean_tfidf = np.array(level_tfidf.mean(axis=0)).flatten()\n",
    "        \n",
    "        # ä¸Šä½kå€‹ã®é‡è¦å˜èªã‚’å–å¾—\n",
    "        top_indices = mean_tfidf.argsort()[-top_k:][::-1]\n",
    "        top_words = [(feature_names[i], mean_tfidf[i]) for i in top_indices]\n",
    "        \n",
    "        health_name = {1: 'â… ', 2: 'â…¡', 3: 'â…¢', 4: 'â…£', 5: 'â…¤'}[level]\n",
    "        results[health_name] = top_words\n",
    "        \n",
    "        print(f\"\\n=== HealthLevel {health_name} ã®é‡è¦å˜èª (Top {top_k}) ===\")\n",
    "        for i, (word, score) in enumerate(top_words):\n",
    "            print(f\"  {i+1:2d}. {word}: {score:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# é‡è¦å˜èªåˆ†æã®å®Ÿè¡Œ\n",
    "important_words = analyze_important_words_by_health_level(\n",
    "    X_train_tfidf, y_train, feature_names, top_k=15\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
