"""
ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
æ©‹æ¢ç‚¹æ¤œãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬çš„ãªå‰å‡¦ç†ã‚’è¡Œã†
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Tuple, Union
import re
import warnings
warnings.filterwarnings('ignore')

class InspectionDataLoader:
    """æ©‹æ¢ç‚¹æ¤œãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, data_dir: str):
        """
        Parameters:
        -----------
        data_dir : str
            ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        """
        self.data_dir = Path(data_dir)
        self.raw_data = None
        self.processed_data = None
        
    def load_data(self, file_patterns: List[str] = None) -> pd.DataFrame:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€çµåˆã™ã‚‹
        
        Parameters:
        -----------
        file_patterns : List[str], optional
            èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‚Noneã®å ´åˆã¯å…¨CSVãƒ•ã‚¡ã‚¤ãƒ«
            
        Returns:
        --------
        pd.DataFrame
            çµåˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        if file_patterns is None:
            csv_files = list(self.data_dir.glob("*.csv"))
        else:
            csv_files = []
            for pattern in file_patterns:
                csv_files.extend(list(self.data_dir.glob(pattern)))
        
        if not csv_files:
            raise FileNotFoundError(f"No CSV files found in {self.data_dir}")
        
        dfs = []
        for file_path in csv_files:
            print(f"Loading: {file_path.name}")
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            df['source_file'] = file_path.name
            dfs.append(df)
        
        self.raw_data = pd.concat(dfs, ignore_index=True)
        print(f"Total records loaded: {len(self.raw_data)}")
        
        return self.raw_data
    
    def get_basic_info(self) -> dict:
        """ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—"""
        if self.raw_data is None:
            raise ValueError("Data not loaded. Call load_data() first.")
        
        info = {
            'total_records': len(self.raw_data),
            'columns': list(self.raw_data.columns),
            'health_level_distribution': self.raw_data['HealthLevel'].value_counts().to_dict(),
            'missing_values': self.raw_data.isnull().sum().to_dict(),
            'unique_bridges': self.raw_data['BridgeID'].nunique(),
            'date_range': {
                'min': self.raw_data['InspectionYMD'].min(),
                'max': self.raw_data['InspectionYMD'].max()
            }
        }
        
        return info
    
    def basic_preprocessing(self) -> pd.DataFrame:
        """åŸºæœ¬çš„ãªå‰å‡¦ç†ã‚’å®Ÿè¡Œ"""
        if self.raw_data is None:
            raise ValueError("Data not loaded. Call load_data() first.")
        
        df = self.raw_data.copy()
        
        # æ—¥ä»˜å‹ã«å¤‰æ›
        df['InspectionYMD'] = pd.to_datetime(df['InspectionYMD'])
        
        # å¹´æœˆã®æŠ½å‡º
        df['inspection_year'] = df['InspectionYMD'].dt.year
        df['inspection_month'] = df['InspectionYMD'].dt.month
        
        # DamageRankã®ã‚«ãƒ†ã‚´ãƒªåŒ–
        df['damage_rank_encoded'] = df['DamageRank'].map({'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5})
        
        # HealthLevelã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆIIIä»¥ä¸Šã‚’Repair-requirementã‚¯ãƒ©ã‚¹ã«çµ±åˆï¼‰
        def encode_health_level(level):
            if level == 'â… ':
                return 1
            elif level == 'â…¡':
                return 2
            elif level in ['â…¢', 'â…£', 'â…¤']:
                return 3  # Repair-requirement ã‚¯ãƒ©ã‚¹
            else:
                return None  # Nã‚„ãã®ä»–ã®å€¤ã¯é™¤å¤–
        
        df['health_level_encoded'] = df['HealthLevel'].apply(encode_health_level)
        
        # 'N'ãƒ¬ãƒ™ãƒ«ï¼ˆè©•ä¾¡å¯¾è±¡å¤–ï¼‰ã‚’é™¤å¤–
        df = df[df['health_level_encoded'].notna()].copy()
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        df['diagnosis_cleaned'] = df['Diagnosis'].apply(self._clean_text)
        df['damage_comment_cleaned'] = df['DamageComment'].apply(self._clean_text)
        
        # æ•°å€¤çš„ç‰¹å¾´é‡ã®æŠ½å‡ºï¼ˆã²ã³å‰²ã‚Œå¹…ã€é¢ç©ãªã©ï¼‰
        df['crack_width'] = df['DamageComment'].apply(self._extract_crack_width)
        df['area_measurement'] = df['DamageComment'].apply(self._extract_area)
        
        self.processed_data = df
        print(f"Preprocessing completed. Shape: {df.shape}")
        
        return df
    
    def _clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if pd.isna(text):
            return ""
        
        # å…¨è§’æ•°å­—ã‚’åŠè§’ã«å¤‰æ›
        text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))
        
        # å…¨è§’è‹±å­—ã‚’åŠè§’ã«å¤‰æ›
        text = text.translate(str.maketrans('ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½š',
                                             'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'))
        
        # æ”¹è¡Œæ–‡å­—ã®é™¤å»
        text = text.replace('\n', ' ').replace('\r', ' ')
        
        # é€£ç¶šã™ã‚‹ç©ºç™½ã®æ­£è¦åŒ–
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _extract_crack_width(self, text: str) -> float:
        """ã²ã³å‰²ã‚Œå¹…ã®æ•°å€¤ã‚’æŠ½å‡ºï¼ˆmmå˜ä½ï¼‰"""
        if pd.isna(text):
            return np.nan
        
        # ã²ã³å‰²ã‚Œå¹…ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        patterns = [
            r'ã²ã³ã‚ã‚Œå¹…[\s]*([0-9.]+)[\s]*mm',
            r'ã²ã³å‰²ã‚Œå¹…[\s]*([0-9.]+)[\s]*mm',
            r'å¹…[\s]*([0-9.]+)[\s]*mm',
            r'([0-9.]+)[\s]*mm'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    return float(match.group(1))
                except ValueError:
                    continue
        
        return np.nan
    
    def _extract_area(self, text: str) -> float:
        """é¢ç©ã®æ•°å€¤ã‚’æŠ½å‡ºï¼ˆmÃ—mã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
        if pd.isna(text):
            return np.nan
        
        # é¢ç©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢ (ä¾‹: 0.8mÃ—0.2m)
        pattern = r'([0-9.]+)mÃ—([0-9.]+)m'
        match = re.search(pattern, text)
        
        if match:
            try:
                width = float(match.group(1))
                height = float(match.group(2))
                return width * height
            except ValueError:
                return np.nan
        
        return np.nan
    
    def create_aggregated_features(self, use_full_data: bool = False) -> pd.DataFrame:
        """æ©‹æ¢ãƒ¬ãƒ™ãƒ«ã§ã®é›†ç´„ç‰¹å¾´é‡ã‚’ä½œæˆ
        
        Parameters:
        -----------
        use_full_data : bool, default=False
            Trueã®å ´åˆã€å€‹åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã§ã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼ˆ9753ä»¶ï¼‰
            Falseã®å ´åˆã€æ©‹æ¢åˆ¥é›†ç´„ç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼ˆ276ä»¶ï¼‰
        """
        if self.processed_data is None:
            raise ValueError("Data not preprocessed. Call basic_preprocessing() first.")
        
        if use_full_data:
            print("ğŸš€ ãƒ•ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ¼ãƒ‰: å€‹åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ï¼ˆ9753ä»¶ï¼‰ã§å­¦ç¿’")
            # å€‹åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã§ã®ç‰¹å¾´é‡
            full_data = self.processed_data.copy()
            
            # HealthLevelã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            def encode_health_level(level):
                if level == 'â… ':
                    return 1
                elif level == 'â…¡':
                    return 2
                elif level in ['â…¢', 'â…£', 'â…¤']:
                    return 3  # Repair-requirement ã‚¯ãƒ©ã‚¹
                else:
                    return None
            
            full_data['health_level_encoded'] = full_data['HealthLevel'].apply(encode_health_level)
            full_data = full_data[full_data['health_level_encoded'].notna()].copy()
            
            # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®æº–å‚™
            full_data['combined_text'] = (
                full_data['diagnosis_cleaned'].fillna('') + ' ' + 
                full_data['damage_comment_cleaned'].fillna('')
            ).str.strip()
            
            # åŸºæœ¬çš„ãªæ•°å€¤ç‰¹å¾´é‡
            feature_columns = [
                'BridgeID', 'health_level_encoded', 'DamageRank', 
                'crack_width', 'area_measurement', 'combined_text'
            ]
            
            # æ•°å€¤ç‰¹å¾´é‡ã®æ¬ æå€¤å‡¦ç†
            full_data['crack_width'] = full_data['crack_width'].fillna(0)
            full_data['area_measurement'] = full_data['area_measurement'].fillna(0)
            full_data['DamageRank'] = pd.to_numeric(full_data['DamageRank'], errors='coerce').fillna(1)
            
            result_data = full_data[feature_columns].copy()
            print(f"ãƒ•ãƒ«ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {result_data.shape}")
            print(f"HealthLevelåˆ†å¸ƒ:")
            print(full_data['HealthLevel'].value_counts())
            
            return result_data
        
        else:
            print("ğŸ“Š é›†ç´„ãƒ¢ãƒ¼ãƒ‰: æ©‹æ¢åˆ¥é›†ç´„ï¼ˆ276ä»¶ï¼‰ã§å­¦ç¿’")
            # æ©‹æ¢Ã—è¨ºæ–­æ—¥ãƒ¬ãƒ™ãƒ«ã§ã®é›†ç´„ï¼ˆå…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
            agg_features = self.processed_data.groupby(['BridgeID', 'BridgeName', 'InspectionYMD', 'HealthLevel']).agg({
                'DiagnosisID': 'nunique',  # è¨ºæ–­é …ç›®æ•°
                'DamageID': 'nunique',     # æå‚·æ•°
                'damage_rank_encoded': ['mean', 'max'],  # æå‚·ãƒ©ãƒ³ã‚¯ã®å¹³å‡ãƒ»æœ€å¤§
                'crack_width': ['mean', 'max', 'count'],  # ã²ã³å‰²ã‚Œå¹…ã®çµ±è¨ˆ
                'area_measurement': ['sum', 'max'],       # é¢ç©ã®çµ±è¨ˆ
                'Diagnosis': lambda x: ' '.join(x.dropna().astype(str).unique()),  # è¨ºæ–­ãƒ†ã‚­ã‚¹ãƒˆã®çµåˆ
                'DamageComment': lambda x: ' '.join(x.dropna().astype(str).unique())  # æå‚·ã‚³ãƒ¡ãƒ³ãƒˆã®çµåˆ
            }).reset_index()
            
            # ã‚«ãƒ©ãƒ åã®æ•´ç†
            agg_features.columns = [
                'BridgeID', 'BridgeName', 'InspectionYMD', 'HealthLevel',
                'diagnosis_count', 'damage_count',
                'damage_rank_mean', 'damage_rank_max',
                'crack_width_mean', 'crack_width_max', 'crack_width_count',
                'area_sum', 'area_max',
                'diagnosis_text', 'damage_comment_text'
            ]
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            agg_features['diagnosis_text'] = agg_features['diagnosis_text'].apply(self._clean_text)
            agg_features['damage_comment_text'] = agg_features['damage_comment_text'].apply(self._clean_text)
            
            return agg_features

def main():
    """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    loader = InspectionDataLoader("../1_inspection-dataset")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    raw_data = loader.load_data()
    print("\n=== Basic Info ===")
    info = loader.get_basic_info()
    for key, value in info.items():
        print(f"{key}: {value}")
    
    # å‰å‡¦ç†å®Ÿè¡Œ
    print("\n=== Preprocessing ===")
    processed_data = loader.basic_preprocessing()
    
    # é›†ç´„ç‰¹å¾´é‡ä½œæˆ
    print("\n=== Aggregated Features ===")
    agg_data = loader.create_aggregated_features()
    print(f"Aggregated data shape: {agg_data.shape}")
    print(agg_data.head())

if __name__ == "__main__":
    main()